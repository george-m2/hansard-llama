{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "!pip install \"unsloth\"\n",
    "!pip install \"gdown\" # not needed for colab, DL from gdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "import os\n",
    "\n",
    "def main():\n",
    "    # ===================================================================\n",
    "    # 5-6-hour run on 1x RTX 4090:\n",
    "    # Batch size: 8\n",
    "    # Gradient: 2\n",
    "    # 5700 steps\n",
    "    # 4bit quantization\n",
    "    # 16 LoRA rank\n",
    "    # 16 LoRA alpha\n",
    "    # 50 warmup steps\n",
    "    # 25 logging steps\n",
    "    # 1000 save steps\n",
    "    # Learning rate: 2e-4\n",
    "    # ===================================================================\n",
    "    max_seq_length = 1024\n",
    "    dtype = None\n",
    "    load_in_4bit = True\n",
    "\n",
    "    print(\"Loading the base model...\")\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "        device_map = {\"\": torch.cuda.current_device()},\n",
    "    )\n",
    "\n",
    "    print(\"Configuring the model for LoRA...\")\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r = 16,\n",
    "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        lora_alpha = 16,\n",
    "        lora_dropout = 0,\n",
    "        bias = \"none\",\n",
    "        use_gradient_checkpointing = True,\n",
    "        random_state = 3407,\n",
    "    )\n",
    "\n",
    "    def format_prompts_for_batch(examples):\n",
    "        instructions = examples[\"instruction\"]\n",
    "        outputs = examples[\"output\"]\n",
    "        if \"system\" in examples:\n",
    "            systems = examples[\"system\"]\n",
    "        else:\n",
    "            systems = [\"You are a helpful assistant.\"] * len(instructions)\n",
    "        texts = []\n",
    "        for instruction, output, system in zip(instructions, outputs, systems):\n",
    "            text = (f\"<|start_header_id|>system<|end_header_id|>\\\\n{system}<|eot_id|><|start_header_id|>user<|end_header_id|>\\\\n{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\\\n{output}<|eot_id|>\")\n",
    "            texts.append(text)\n",
    "        return { \"text\" : texts, }\n",
    "\n",
    "    processed_dataset_path = \"tokenized_hansard_dataset\"\n",
    "    raw_dataset_path = \"training_data_unified.jsonl\"\n",
    "    output_dir = \"outputs\"\n",
    "    final_model_path = \"lora_model\"\n",
    "\n",
    "    if os.path.exists(processed_dataset_path):\n",
    "        print(f\"Found pre-processed dataset at '{processed_dataset_path}'\")\n",
    "        formatted_dataset = load_from_disk(processed_dataset_path)\n",
    "        print(\"Dataset loaded successfully.\")\n",
    "    else:\n",
    "        print(f\"No pre-processed dataset found. Starting full processing...\")\n",
    "        raw_dataset = load_dataset(\"json\", data_files=raw_dataset_path, split=\"train\")\n",
    "        formatted_dataset = raw_dataset.map(format_prompts_for_batch, batched=True)\n",
    "        print(f\"Processing complete. Saving dataset to '{processed_dataset_path}'...\")\n",
    "        formatted_dataset.save_to_disk(processed_dataset_path)\n",
    "        print(\"Dataset saved successfully\")\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model = model,\n",
    "        tokenizer = tokenizer,\n",
    "        train_dataset = formatted_dataset,\n",
    "        dataset_text_field = \"text\",\n",
    "        max_seq_length = max_seq_length,\n",
    "        packing = True,\n",
    "        args = TrainingArguments(\n",
    "            per_device_train_batch_size = 8,\n",
    "            gradient_accumulation_steps = 2,\n",
    "            warmup_steps = 50,\n",
    "            max_steps = 5700, \n",
    "            logging_steps = 25,\n",
    "            save_strategy = \"steps\",\n",
    "            save_steps = 1000, # Checkpoint every 1000 steps\n",
    "            learning_rate = 2e-4,\n",
    "            fp16 = not torch.cuda.is_bf16_supported(),\n",
    "            bf16 = torch.cuda.is_bf16_supported(),\n",
    "            optim = \"adamw_8bit\",\n",
    "            weight_decay = 0.01,\n",
    "            lr_scheduler_type = \"linear\",\n",
    "            seed = 3407,\n",
    "            output_dir = output_dir,\n",
    "            report_to = \"none\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    print(f\"Starting training run for {5700} steps...\")\n",
    "    # automatically resume from the latest checkpoint if found\n",
    "    trainer.train()\n",
    "    print(\"Training complete.\")\n",
    "    print(\"Saving LoRA adapter...\")\n",
    "    model.save_pretrained(final_model_path)\n",
    "    print(f\"\\\\nModel adapter saved to '{final_model_path}'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipex flag is deprecated, will be removed in Accelerate v1.10. From 2.7.0, PyTorch has all needed optimizations for Intel CPU and XPU.\n",
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "Loading the base model...\n",
      "==((====))==  Unsloth 2025.6.5: Fast Llama patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.643 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Configuring the model for LoRA...\n",
      "Unsloth 2025.6.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n",
      "Found pre-processed dataset at 'tokenized_hansard_dataset'\n",
      "Dataset loaded successfully.\n",
      "Starting training run for 5700 steps...\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,517,832 | Num Epochs = 1 | Total steps = 5,700\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 2 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 41,943,040/8,000,000,000 (0.52% trained)\n",
      "{'loss': 2.4617, 'grad_norm': 6.405088901519775, 'learning_rate': 9.6e-05, 'epoch': 0.0}\n",
      "{'loss': 0.2903, 'grad_norm': 0.08837438374757767, 'learning_rate': 0.000196, 'epoch': 0.0}\n",
      "{'loss': 0.1916, 'grad_norm': 0.025282127782702446, 'learning_rate': 0.0001991504424778761, 'epoch': 0.0}\n",
      "{'loss': 0.205, 'grad_norm': 0.02392497844994068, 'learning_rate': 0.00019826548672566372, 'epoch': 0.0}\n",
      "{'loss': 0.2118, 'grad_norm': 0.0076134526170790195, 'learning_rate': 0.00019738053097345133, 'epoch': 0.0}\n",
      "{'loss': 0.2088, 'grad_norm': 0.011027485132217407, 'learning_rate': 0.00019649557522123895, 'epoch': 0.0}\n",
      "{'loss': 0.2011, 'grad_norm': 0.02226654253900051, 'learning_rate': 0.00019561061946902656, 'epoch': 0.0}\n",
      "{'loss': 0.2099, 'grad_norm': 0.014521025121212006, 'learning_rate': 0.00019472566371681418, 'epoch': 0.0}\n",
      "{'loss': 0.2018, 'grad_norm': 0.010974316857755184, 'learning_rate': 0.0001938407079646018, 'epoch': 0.0}\n",
      "{'loss': 0.2098, 'grad_norm': 0.00809813104569912, 'learning_rate': 0.0001929557522123894, 'epoch': 0.0}\n",
      "{'loss': 0.2086, 'grad_norm': 0.0123305544257164, 'learning_rate': 0.0001911858407079646, 'epoch': 0.0}\n",
      "{'loss': 0.209, 'grad_norm': 0.00354215526022017, 'learning_rate': 0.00019030088495575222, 'epoch': 0.0}\n",
      "{'loss': 0.1939, 'grad_norm': 0.01576586626470089, 'learning_rate': 0.00018941592920353984, 'epoch': 0.0}\n",
      "{'loss': 0.1943, 'grad_norm': 0.005261261481791735, 'learning_rate': 0.00018764601769911504, 'epoch': 0.0}\n",
      "{'loss': 0.2005, 'grad_norm': 0.00587467011064291, 'learning_rate': 0.00018676106194690265, 'epoch': 0.0}\n",
      "{'loss': 0.1905, 'grad_norm': 0.006921923719346523, 'learning_rate': 0.00018587610619469027, 'epoch': 0.0}\n",
      "{'loss': 0.1922, 'grad_norm': 0.024130435660481453, 'learning_rate': 0.00018499115044247788, 'epoch': 0.01}\n",
      "{'loss': 0.1895, 'grad_norm': 0.005214404314756393, 'learning_rate': 0.0001841061946902655, 'epoch': 0.01}\n",
      "{'loss': 0.2088, 'grad_norm': 0.007705027237534523, 'learning_rate': 0.00018322123893805309, 'epoch': 0.01}\n",
      "{'loss': 0.2208, 'grad_norm': 0.0160817988216877, 'learning_rate': 0.0001823362831858407, 'epoch': 0.01}\n",
      "{'loss': 0.1907, 'grad_norm': 0.03128688409924507, 'learning_rate': 0.00018145132743362834, 'epoch': 0.01}\n",
      "{'loss': 0.2119, 'grad_norm': 0.01955920085310936, 'learning_rate': 0.00017968141592920354, 'epoch': 0.01}\n",
      "{'loss': 0.1873, 'grad_norm': 0.02237873524427414, 'learning_rate': 0.00017879646017699116, 'epoch': 0.01}\n",
      "{'loss': 0.1902, 'grad_norm': 0.01034713163971901, 'learning_rate': 0.00017791150442477877, 'epoch': 0.01}\n",
      "{'loss': 0.1968, 'grad_norm': 0.03205113857984543, 'learning_rate': 0.0001770265486725664, 'epoch': 0.01}\n",
      "{'loss': 0.1996, 'grad_norm': 0.044121820479631424, 'learning_rate': 0.000176141592920354, 'epoch': 0.01}\n",
      "{'loss': 0.1835, 'grad_norm': 0.00667860871180892, 'learning_rate': 0.0001752566371681416, 'epoch': 0.01}\n",
      "{'loss': 0.2086, 'grad_norm': 0.003791305236518383, 'learning_rate': 0.0001743716814159292, 'epoch': 0.01}\n",
      "{'loss': 0.2004, 'grad_norm': 0.0033756806515157223, 'learning_rate': 0.00017348672566371682, 'epoch': 0.01}\n",
      "{'loss': 0.1871, 'grad_norm': 0.002875257283449173, 'learning_rate': 0.00017260176991150443, 'epoch': 0.01}\n",
      "{'loss': 0.2039, 'grad_norm': 0.004829525016248226, 'learning_rate': 0.00017171681415929205, 'epoch': 0.01}\n",
      "{'loss': 0.2211, 'grad_norm': 0.008718198165297508, 'learning_rate': 0.00017083185840707964, 'epoch': 0.01}\n",
      "{'loss': 0.2074, 'grad_norm': 0.004811842460185289, 'learning_rate': 0.00016994690265486725, 'epoch': 0.01}\n",
      "{'loss': 0.2197, 'grad_norm': 0.022430390119552612, 'learning_rate': 0.00016906194690265486, 'epoch': 0.01}\n",
      "{'loss': 0.2027, 'grad_norm': 0.017972130328416824, 'learning_rate': 0.00016817699115044248, 'epoch': 0.01}\n",
      "{'loss': 0.1834, 'grad_norm': 0.0029251673258841038, 'learning_rate': 0.00016729203539823012, 'epoch': 0.01}\n",
      "{'loss': 0.2392, 'grad_norm': 0.013850797899067402, 'learning_rate': 0.0001664070796460177, 'epoch': 0.01}\n",
      " 18%|██████▏                            | 1000/5700 [1:12:50<4:32:18,  3.48s/it]/venv/main/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: f553f2a4-b83f-4997-8ec0-cfba1d711d5d)') - silently ignoring the lookup for the file config.json in unsloth/llama-3-8b-bnb-4bit.\n",
      "  warnings.warn(\n",
      "/venv/main/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in unsloth/llama-3-8b-bnb-4bit - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "{'loss': 0.2166, 'grad_norm': 0.005120015703141689, 'learning_rate': 0.00016552212389380532, 'epoch': 0.01}\n",
      "{'loss': 0.2063, 'grad_norm': 0.004316783510148525, 'learning_rate': 0.00016463716814159294, 'epoch': 0.01}\n",
      "{'loss': 0.2043, 'grad_norm': 0.01795264706015587, 'learning_rate': 0.00016375221238938055, 'epoch': 0.01}\n",
      "{'loss': 0.2196, 'grad_norm': 0.004196154419332743, 'learning_rate': 0.00016286725663716817, 'epoch': 0.01}\n",
      "{'loss': 0.1935, 'grad_norm': 0.00670880451798439, 'learning_rate': 0.00016198230088495575, 'epoch': 0.01}\n",
      "{'loss': 0.2037, 'grad_norm': 0.41099047660827637, 'learning_rate': 0.00016109734513274337, 'epoch': 0.01}\n",
      "{'loss': 0.2019, 'grad_norm': 0.021511362865567207, 'learning_rate': 0.00016021238938053098, 'epoch': 0.01}\n",
      "{'loss': 0.1989, 'grad_norm': 0.004995991010218859, 'learning_rate': 0.0001593274336283186, 'epoch': 0.01}\n",
      "{'loss': 0.2086, 'grad_norm': 0.00448829447850585, 'learning_rate': 0.0001584424778761062, 'epoch': 0.01}\n",
      "{'loss': 0.2019, 'grad_norm': 0.005247603636234999, 'learning_rate': 0.0001575575221238938, 'epoch': 0.01}\n",
      "{'loss': 0.2068, 'grad_norm': 0.005732581485062838, 'learning_rate': 0.0001566725663716814, 'epoch': 0.01}\n",
      "{'loss': 0.2187, 'grad_norm': 0.014379780739545822, 'learning_rate': 0.00015578761061946903, 'epoch': 0.01}\n",
      "{'loss': 0.211, 'grad_norm': 0.008173015899956226, 'learning_rate': 0.00015490265486725664, 'epoch': 0.01}\n",
      "{'loss': 0.2133, 'grad_norm': 0.008824988268315792, 'learning_rate': 0.00015401769911504426, 'epoch': 0.01}\n",
      "{'loss': 0.222, 'grad_norm': 0.008397184312343597, 'learning_rate': 0.00015313274336283187, 'epoch': 0.01}\n",
      "{'loss': 0.228, 'grad_norm': 0.007191766519099474, 'learning_rate': 0.00015224778761061949, 'epoch': 0.01}\n",
      "{'loss': 0.2105, 'grad_norm': 0.006537256296724081, 'learning_rate': 0.0001513628318584071, 'epoch': 0.02}\n",
      "{'loss': 0.2019, 'grad_norm': 0.07493255287408829, 'learning_rate': 0.00015047787610619472, 'epoch': 0.02}\n",
      "{'loss': 0.2002, 'grad_norm': 0.004553221631795168, 'learning_rate': 0.0001495929203539823, 'epoch': 0.02}\n",
      "{'loss': 0.1917, 'grad_norm': 0.003958908841013908, 'learning_rate': 0.00014782300884955753, 'epoch': 0.02}\n",
      "{'loss': 0.2047, 'grad_norm': 0.06303331255912781, 'learning_rate': 0.00014693805309734515, 'epoch': 0.02}\n",
      "{'loss': 0.2158, 'grad_norm': 0.006523814983665943, 'learning_rate': 0.00014605309734513276, 'epoch': 0.02}\n",
      "{'loss': 0.1962, 'grad_norm': 0.0021637831814587116, 'learning_rate': 0.00014428318584070796, 'epoch': 0.02}\n",
      "{'loss': 0.1996, 'grad_norm': 0.003267340362071991, 'learning_rate': 0.00014339823008849558, 'epoch': 0.02}\n",
      "{'loss': 0.2065, 'grad_norm': 0.005299887619912624, 'learning_rate': 0.0001425132743362832, 'epoch': 0.02}\n",
      "{'loss': 0.2079, 'grad_norm': 0.006704547442495823, 'learning_rate': 0.0001416283185840708, 'epoch': 0.02}\n",
      "{'loss': 0.198, 'grad_norm': 0.0024376309011131525, 'learning_rate': 0.0001407433628318584, 'epoch': 0.02}\n",
      "{'loss': 0.1998, 'grad_norm': 0.004258017521351576, 'learning_rate': 0.000139858407079646, 'epoch': 0.02}\n",
      "{'loss': 0.2072, 'grad_norm': 0.0035576154477894306, 'learning_rate': 0.00013897345132743365, 'epoch': 0.02}\n",
      "{'loss': 0.2075, 'grad_norm': 0.003849624888971448, 'learning_rate': 0.00013808849557522126, 'epoch': 0.02}\n",
      "{'loss': 0.2215, 'grad_norm': 0.008569261059165001, 'learning_rate': 0.00013720353982300885, 'epoch': 0.02}\n",
      "{'loss': 0.2099, 'grad_norm': 0.0055763572454452515, 'learning_rate': 0.00013543362831858408, 'epoch': 0.02}\n",
      "{'loss': 0.2074, 'grad_norm': 0.008054058067500591, 'learning_rate': 0.0001345486725663717, 'epoch': 0.02}\n",
      "{'loss': 0.1974, 'grad_norm': 0.005622080992907286, 'learning_rate': 0.0001336637168141593, 'epoch': 0.02}\n",
      "{'loss': 0.2093, 'grad_norm': 0.01087487954646349, 'learning_rate': 0.0001327787610619469, 'epoch': 0.02}\n",
      "{'loss': 0.2036, 'grad_norm': 0.006200375966727734, 'learning_rate': 0.0001318938053097345, 'epoch': 0.02}\n",
      "{'loss': 0.208, 'grad_norm': 0.0033529719803482294, 'learning_rate': 0.00013012389380530974, 'epoch': 0.02}\n",
      "{'loss': 0.1966, 'grad_norm': 0.0030922843143343925, 'learning_rate': 0.00012923893805309736, 'epoch': 0.02}\n",
      "{'loss': 0.1986, 'grad_norm': 0.004165954422205687, 'learning_rate': 0.00012835398230088494, 'epoch': 0.02}\n",
      "{'loss': 0.1914, 'grad_norm': 0.03889892250299454, 'learning_rate': 0.00012746902654867256, 'epoch': 0.02}\n",
      "{'loss': 0.2076, 'grad_norm': 0.0029216494876891375, 'learning_rate': 0.00012658407079646017, 'epoch': 0.02}\n",
      "{'loss': 0.1966, 'grad_norm': 0.004122002981603146, 'learning_rate': 0.0001256991150442478, 'epoch': 0.02}\n",
      "{'loss': 0.1898, 'grad_norm': 0.002300238236784935, 'learning_rate': 0.00012481415929203543, 'epoch': 0.02}\n",
      "{'loss': 0.2167, 'grad_norm': 0.002598272170871496, 'learning_rate': 0.00012392920353982302, 'epoch': 0.02}\n",
      "{'loss': 0.2096, 'grad_norm': 0.006229513790458441, 'learning_rate': 0.00012304424778761063, 'epoch': 0.02}\n",
      "{'loss': 0.2009, 'grad_norm': 0.0061104875057935715, 'learning_rate': 0.00012215929203539824, 'epoch': 0.02}\n",
      "{'loss': 0.2473, 'grad_norm': 0.004329426679760218, 'learning_rate': 0.00012127433628318585, 'epoch': 0.02}\n",
      "{'loss': 0.2166, 'grad_norm': 0.003936028573662043, 'learning_rate': 0.00012038938053097346, 'epoch': 0.02}\n",
      "{'loss': 0.1812, 'grad_norm': 0.017484420910477638, 'learning_rate': 0.00011950442477876107, 'epoch': 0.02}\n",
      "{'loss': 0.1979, 'grad_norm': 0.0031447731889784336, 'learning_rate': 0.00011861946902654868, 'epoch': 0.02}\n",
      "{'loss': 0.1928, 'grad_norm': 0.0032429106067866087, 'learning_rate': 0.00011773451327433629, 'epoch': 0.03}\n",
      "{'loss': 0.2165, 'grad_norm': 0.006690345238894224, 'learning_rate': 0.0001168495575221239, 'epoch': 0.03}\n",
      "{'loss': 0.2146, 'grad_norm': 0.0023831299040466547, 'learning_rate': 0.0001159646017699115, 'epoch': 0.03}\n",
      "{'loss': 0.1855, 'grad_norm': 0.001761655556038022, 'learning_rate': 0.00011507964601769912, 'epoch': 0.03}\n",
      "{'loss': 0.1971, 'grad_norm': 0.005020547192543745, 'learning_rate': 0.00011419469026548672, 'epoch': 0.03}\n",
      "{'loss': 0.198, 'grad_norm': 0.00772798852995038, 'learning_rate': 0.00011242477876106195, 'epoch': 0.03}\n",
      "{'loss': 0.1949, 'grad_norm': 0.0047364616766572, 'learning_rate': 0.00011153982300884955, 'epoch': 0.03}\n",
      "{'loss': 0.2088, 'grad_norm': 0.0021438489202409983, 'learning_rate': 0.00011065486725663717, 'epoch': 0.03}\n",
      "{'loss': 0.1862, 'grad_norm': 0.003133042249828577, 'learning_rate': 0.0001097699115044248, 'epoch': 0.03}\n",
      "{'loss': 0.2095, 'grad_norm': 0.010098742321133614, 'learning_rate': 0.00010888495575221241, 'epoch': 0.03}\n",
      "{'loss': 0.2002, 'grad_norm': 0.0018472884548828006, 'learning_rate': 0.00010800000000000001, 'epoch': 0.03}\n",
      "{'loss': 0.2282, 'grad_norm': 0.006548087112605572, 'learning_rate': 0.00010711504424778762, 'epoch': 0.03}\n",
      "{'loss': 0.2057, 'grad_norm': 0.006975229829549789, 'learning_rate': 0.00010534513274336284, 'epoch': 0.03}\n",
      "{'loss': 0.1987, 'grad_norm': 0.0020915663335472345, 'learning_rate': 0.00010446017699115045, 'epoch': 0.03}\n",
      "{'loss': 0.1924, 'grad_norm': 0.0030411644838750362, 'learning_rate': 0.00010357522123893806, 'epoch': 0.03}\n",
      "{'loss': 0.2116, 'grad_norm': 0.006422273814678192, 'learning_rate': 0.00010269026548672567, 'epoch': 0.03}\n",
      "{'loss': 0.2059, 'grad_norm': 0.006184334866702557, 'learning_rate': 0.00010180530973451327, 'epoch': 0.03}\n",
      "{'loss': 0.1874, 'grad_norm': 0.0028974656015634537, 'learning_rate': 0.00010092035398230089, 'epoch': 0.03}\n",
      "{'loss': 0.2132, 'grad_norm': 0.008244828321039677, 'learning_rate': 9.915044247787611e-05, 'epoch': 0.03}\n",
      "{'loss': 0.1898, 'grad_norm': 0.0014537099050357938, 'learning_rate': 9.826548672566373e-05, 'epoch': 0.03}\n",
      "{'loss': 0.2052, 'grad_norm': 0.006628766655921936, 'learning_rate': 9.738053097345133e-05, 'epoch': 0.03}\n",
      "{'loss': 0.2127, 'grad_norm': 0.006715783383697271, 'learning_rate': 9.649557522123894e-05, 'epoch': 0.03}\n",
      "{'loss': 0.2057, 'grad_norm': 0.003994596190750599, 'learning_rate': 9.561061946902656e-05, 'epoch': 0.03}\n",
      " 53%|██████████████████▍                | 3000/5700 [3:37:11<3:11:43,  4.26s/it]/venv/main/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 28d748bf-baef-4745-b892-ee5de99ac879)') - silently ignoring the lookup for the file config.json in unsloth/llama-3-8b-bnb-4bit.\n",
      "  warnings.warn(\n",
      "/venv/main/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in unsloth/llama-3-8b-bnb-4bit - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "{'loss': 0.1994, 'grad_norm': 0.005050936713814735, 'learning_rate': 9.472566371681416e-05, 'epoch': 0.03}\n",
      "{'loss': 0.2075, 'grad_norm': 0.009166100062429905, 'learning_rate': 9.384070796460177e-05, 'epoch': 0.03}\n",
      "{'loss': 0.1963, 'grad_norm': 0.005289874505251646, 'learning_rate': 9.295575221238938e-05, 'epoch': 0.03}\n",
      "{'loss': 0.2054, 'grad_norm': 0.0021517607383430004, 'learning_rate': 9.2070796460177e-05, 'epoch': 0.03}\n",
      "{'loss': 0.2087, 'grad_norm': 0.006243978627026081, 'learning_rate': 9.11858407079646e-05, 'epoch': 0.03}\n",
      "{'loss': 0.1957, 'grad_norm': 0.0035236298572272062, 'learning_rate': 9.030088495575222e-05, 'epoch': 0.03}\n",
      "{'loss': 0.2227, 'grad_norm': 0.004005548544228077, 'learning_rate': 8.941592920353983e-05, 'epoch': 0.03}\n",
      "{'loss': 0.2089, 'grad_norm': 0.001272536814212799, 'learning_rate': 8.853097345132743e-05, 'epoch': 0.03}\n",
      "{'loss': 0.1929, 'grad_norm': 0.003427310846745968, 'learning_rate': 8.764601769911505e-05, 'epoch': 0.03}\n",
      "{'loss': 0.21, 'grad_norm': 0.0036294765304774046, 'learning_rate': 8.676106194690265e-05, 'epoch': 0.03}\n",
      "{'loss': 0.1901, 'grad_norm': 0.004499137867242098, 'learning_rate': 8.587610619469026e-05, 'epoch': 0.03}\n",
      "{'loss': 0.1898, 'grad_norm': 0.006776729598641396, 'learning_rate': 8.499115044247788e-05, 'epoch': 0.03}\n",
      "{'loss': 0.2064, 'grad_norm': 0.0036924562882632017, 'learning_rate': 8.41061946902655e-05, 'epoch': 0.04}\n",
      "{'loss': 0.2048, 'grad_norm': 0.003934773150831461, 'learning_rate': 8.233628318584071e-05, 'epoch': 0.04}\n",
      "{'loss': 0.1987, 'grad_norm': 0.002664156723767519, 'learning_rate': 8.145132743362832e-05, 'epoch': 0.04}\n",
      "{'loss': 0.2263, 'grad_norm': 0.007322031073272228, 'learning_rate': 7.968141592920354e-05, 'epoch': 0.04}\n",
      "{'loss': 0.2073, 'grad_norm': 0.0016908393008634448, 'learning_rate': 7.879646017699115e-05, 'epoch': 0.04}\n",
      "{'loss': 0.2107, 'grad_norm': 0.004154485650360584, 'learning_rate': 7.791150442477877e-05, 'epoch': 0.04}\n",
      "{'loss': 0.2128, 'grad_norm': 0.004814798943698406, 'learning_rate': 7.702654867256638e-05, 'epoch': 0.04}\n",
      "{'loss': 0.2015, 'grad_norm': 0.005775768775492907, 'learning_rate': 7.614159292035398e-05, 'epoch': 0.04}\n",
      "{'loss': 0.1904, 'grad_norm': 0.005473549943417311, 'learning_rate': 7.52566371681416e-05, 'epoch': 0.04}\n",
      "{'loss': 0.1902, 'grad_norm': 0.0036211975384503603, 'learning_rate': 7.348672566371681e-05, 'epoch': 0.04}\n",
      "{'loss': 0.1996, 'grad_norm': 0.005057923030108213, 'learning_rate': 7.260176991150443e-05, 'epoch': 0.04}\n",
      "{'loss': 0.2088, 'grad_norm': 0.006491551641374826, 'learning_rate': 7.171681415929203e-05, 'epoch': 0.04}\n",
      "{'loss': 0.2022, 'grad_norm': 0.0033753050956875086, 'learning_rate': 7.083185840707966e-05, 'epoch': 0.04}\n",
      "{'loss': 0.2115, 'grad_norm': 0.003000435885041952, 'learning_rate': 6.994690265486726e-05, 'epoch': 0.04}\n",
      "{'loss': 0.2055, 'grad_norm': 0.003143261419609189, 'learning_rate': 6.906194690265487e-05, 'epoch': 0.04}\n",
      "{'loss': 0.2198, 'grad_norm': 0.009755009785294533, 'learning_rate': 6.817699115044249e-05, 'epoch': 0.04}\n",
      "{'loss': 0.2109, 'grad_norm': 0.0040087648667395115, 'learning_rate': 6.729203539823009e-05, 'epoch': 0.04}\n",
      "{'loss': 0.2156, 'grad_norm': 0.0076473732478916645, 'learning_rate': 6.64070796460177e-05, 'epoch': 0.04}\n",
      "{'loss': 0.1888, 'grad_norm': 0.0035595488734543324, 'learning_rate': 6.55221238938053e-05, 'epoch': 0.04}\n",
      "{'loss': 0.1913, 'grad_norm': 0.0035663507878780365, 'learning_rate': 6.463716814159292e-05, 'epoch': 0.04}\n",
      "{'loss': 0.2076, 'grad_norm': 0.0025506725069135427, 'learning_rate': 6.375221238938053e-05, 'epoch': 0.04}\n",
      "{'loss': 0.1996, 'grad_norm': 0.0029550199396908283, 'learning_rate': 6.286725663716815e-05, 'epoch': 0.04}\n",
      "{'loss': 0.2186, 'grad_norm': 0.0026852753944694996, 'learning_rate': 6.198230088495576e-05, 'epoch': 0.04}\n",
      "{'loss': 0.2132, 'grad_norm': 0.004752120468765497, 'learning_rate': 6.109734513274336e-05, 'epoch': 0.04}\n",
      "{'loss': 0.2162, 'grad_norm': 0.0031681389082223177, 'learning_rate': 6.021238938053098e-05, 'epoch': 0.04}\n",
      "{'loss': 0.2104, 'grad_norm': 0.005380782764405012, 'learning_rate': 5.9327433628318585e-05, 'epoch': 0.04}\n",
      "{'loss': 0.1893, 'grad_norm': 0.004922139458358288, 'learning_rate': 5.844247787610619e-05, 'epoch': 0.04}\n",
      "{'loss': 0.1929, 'grad_norm': 0.0026032233145087957, 'learning_rate': 5.75575221238938e-05, 'epoch': 0.04}\n",
      "{'loss': 0.1873, 'grad_norm': 0.0011634143302217126, 'learning_rate': 5.667256637168142e-05, 'epoch': 0.04}\n",
      "{'loss': 0.2067, 'grad_norm': 0.008738606236875057, 'learning_rate': 5.578761061946903e-05, 'epoch': 0.04}\n",
      "{'loss': 0.1986, 'grad_norm': 0.0033098780550062656, 'learning_rate': 5.4902654867256644e-05, 'epoch': 0.04}\n",
      "{'loss': 0.201, 'grad_norm': 0.0020658359862864017, 'learning_rate': 5.401769911504425e-05, 'epoch': 0.04}\n",
      "{'loss': 0.1925, 'grad_norm': 0.004857433028519154, 'learning_rate': 5.313274336283186e-05, 'epoch': 0.04}\n",
      "{'loss': 0.1864, 'grad_norm': 0.003256740514189005, 'learning_rate': 5.224778761061947e-05, 'epoch': 0.04}\n",
      "{'loss': 0.1965, 'grad_norm': 0.002800751943141222, 'learning_rate': 5.136283185840708e-05, 'epoch': 0.04}\n",
      "{'loss': 0.2025, 'grad_norm': 0.003549412125721574, 'learning_rate': 5.047787610619469e-05, 'epoch': 0.05}\n",
      "{'loss': 0.1908, 'grad_norm': 0.002302797744050622, 'learning_rate': 4.9592920353982305e-05, 'epoch': 0.05}\n",
      "{'loss': 0.2056, 'grad_norm': 0.0017750384286046028, 'learning_rate': 4.870796460176991e-05, 'epoch': 0.05}\n",
      "{'loss': 0.2071, 'grad_norm': 0.0026968521997332573, 'learning_rate': 4.782300884955753e-05, 'epoch': 0.05}\n",
      "{'loss': 0.1921, 'grad_norm': 0.006845013238489628, 'learning_rate': 4.6938053097345135e-05, 'epoch': 0.05}\n",
      "{'loss': 0.1871, 'grad_norm': 0.002681649522855878, 'learning_rate': 4.605309734513274e-05, 'epoch': 0.05}\n",
      "{'loss': 0.2053, 'grad_norm': 0.0013940271455794573, 'learning_rate': 4.516814159292036e-05, 'epoch': 0.05}\n",
      "{'loss': 0.2045, 'grad_norm': 0.002852909965440631, 'learning_rate': 4.428318584070797e-05, 'epoch': 0.05}\n",
      "{'loss': 0.189, 'grad_norm': 0.0012897815322503448, 'learning_rate': 4.339823008849558e-05, 'epoch': 0.05}\n",
      "{'loss': 0.202, 'grad_norm': 0.0025831463281065226, 'learning_rate': 4.251327433628319e-05, 'epoch': 0.05}\n",
      "{'loss': 0.2006, 'grad_norm': 0.0019990205764770508, 'learning_rate': 4.1628318584070795e-05, 'epoch': 0.05}\n",
      "{'loss': 0.2056, 'grad_norm': 0.011781428940594196, 'learning_rate': 4.074336283185841e-05, 'epoch': 0.05}\n",
      "{'loss': 0.1897, 'grad_norm': 0.004168953746557236, 'learning_rate': 3.9858407079646024e-05, 'epoch': 0.05}\n",
      "{'loss': 0.1895, 'grad_norm': 0.006422644015401602, 'learning_rate': 3.897345132743363e-05, 'epoch': 0.05}\n",
      "{'loss': 0.2031, 'grad_norm': 0.00214633671566844, 'learning_rate': 3.808849557522124e-05, 'epoch': 0.05}\n",
      "{'loss': 0.2099, 'grad_norm': 0.00259150518104434, 'learning_rate': 3.7203539823008854e-05, 'epoch': 0.05}\n",
      "{'loss': 0.2073, 'grad_norm': 0.0017678699223324656, 'learning_rate': 3.631858407079646e-05, 'epoch': 0.05}\n",
      "{'loss': 0.2037, 'grad_norm': 0.002273616148158908, 'learning_rate': 3.543362831858407e-05, 'epoch': 0.05}\n",
      "{'loss': 0.2118, 'grad_norm': 0.004141988232731819, 'learning_rate': 3.4548672566371684e-05, 'epoch': 0.05}\n",
      "{'loss': 0.2065, 'grad_norm': 0.0013640159741044044, 'learning_rate': 3.36637168141593e-05, 'epoch': 0.05}\n",
      "{'loss': 0.1965, 'grad_norm': 0.002598423045128584, 'learning_rate': 3.2778761061946906e-05, 'epoch': 0.05}\n",
      "{'loss': 0.207, 'grad_norm': 0.007196126971393824, 'learning_rate': 3.1893805309734514e-05, 'epoch': 0.05}\n",
      "{'loss': 0.2159, 'grad_norm': 0.003195076249539852, 'learning_rate': 3.100884955752212e-05, 'epoch': 0.05}\n",
      "{'loss': 0.2073, 'grad_norm': 0.005986555013805628, 'learning_rate': 3.012389380530974e-05, 'epoch': 0.05}\n",
      "{'loss': 0.2371, 'grad_norm': 0.0019750832580029964, 'learning_rate': 2.9238938053097347e-05, 'epoch': 0.05}\n",
      "{'loss': 0.2043, 'grad_norm': 0.003502946114167571, 'learning_rate': 2.835398230088496e-05, 'epoch': 0.05}\n",
      "{'loss': 0.2002, 'grad_norm': 0.0028952802531421185, 'learning_rate': 2.7469026548672566e-05, 'epoch': 0.05}\n",
      "{'loss': 0.1884, 'grad_norm': 0.0035751855466514826, 'learning_rate': 2.658407079646018e-05, 'epoch': 0.05}\n",
      "{'loss': 0.226, 'grad_norm': 0.001435639918781817, 'learning_rate': 2.569911504424779e-05, 'epoch': 0.05}\n",
      "{'loss': 0.1961, 'grad_norm': 0.00238485774025321, 'learning_rate': 2.48141592920354e-05, 'epoch': 0.05}\n",
      "{'loss': 0.2112, 'grad_norm': 0.0051330202259123325, 'learning_rate': 2.392920353982301e-05, 'epoch': 0.05}\n",
      "{'loss': 0.2026, 'grad_norm': 0.0043014660477638245, 'learning_rate': 2.3044247787610622e-05, 'epoch': 0.05}\n",
      "{'loss': 0.2109, 'grad_norm': 0.0030521864537149668, 'learning_rate': 2.2159292035398233e-05, 'epoch': 0.05}\n",
      "{'loss': 0.1943, 'grad_norm': 0.003932557068765163, 'learning_rate': 2.127433628318584e-05, 'epoch': 0.05}\n",
      "{'loss': 0.2058, 'grad_norm': 0.001778729958459735, 'learning_rate': 2.0389380530973452e-05, 'epoch': 0.05}\n",
      "{'loss': 0.2089, 'grad_norm': 0.0019158129580318928, 'learning_rate': 1.9504424778761063e-05, 'epoch': 0.05}\n",
      "{'loss': 0.2373, 'grad_norm': 0.0017331007402390242, 'learning_rate': 1.861946902654867e-05, 'epoch': 0.05}\n",
      "{'loss': 0.2089, 'grad_norm': 0.0016603294061496854, 'learning_rate': 1.7734513274336285e-05, 'epoch': 0.05}\n",
      "{'loss': 0.2095, 'grad_norm': 0.004006505478173494, 'learning_rate': 1.6849557522123893e-05, 'epoch': 0.06}\n",
      "{'loss': 0.207, 'grad_norm': 0.004254705738276243, 'learning_rate': 1.5964601769911504e-05, 'epoch': 0.06}\n",
      "{'loss': 0.19, 'grad_norm': 0.0029770575929433107, 'learning_rate': 1.5079646017699114e-05, 'epoch': 0.06}\n",
      "{'loss': 0.2138, 'grad_norm': 0.004675313364714384, 'learning_rate': 1.4194690265486727e-05, 'epoch': 0.06}\n",
      "{'loss': 0.2077, 'grad_norm': 0.0016559819923713803, 'learning_rate': 1.3309734513274336e-05, 'epoch': 0.06}\n",
      "{'loss': 0.205, 'grad_norm': 0.0017849039286375046, 'learning_rate': 1.2424778761061947e-05, 'epoch': 0.06}\n",
      "{'loss': 0.1794, 'grad_norm': 0.0029922036919742823, 'learning_rate': 1.1539823008849558e-05, 'epoch': 0.06}\n",
      "{'loss': 0.1926, 'grad_norm': 0.001466703717596829, 'learning_rate': 1.065486725663717e-05, 'epoch': 0.06}\n",
      "{'loss': 0.1846, 'grad_norm': 0.0035346229560673237, 'learning_rate': 9.769911504424779e-06, 'epoch': 0.06}\n",
      "{'loss': 0.2093, 'grad_norm': 0.0023036771453917027, 'learning_rate': 8.88495575221239e-06, 'epoch': 0.06}\n",
      "{'loss': 0.2041, 'grad_norm': 0.003946581389755011, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.06}\n",
      "{'loss': 0.203, 'grad_norm': 0.0037788881454616785, 'learning_rate': 7.115044247787611e-06, 'epoch': 0.06}\n",
      "{'loss': 0.201, 'grad_norm': 0.0016347679775208235, 'learning_rate': 6.230088495575222e-06, 'epoch': 0.06}\n",
      "{'loss': 0.2044, 'grad_norm': 0.0019642296247184277, 'learning_rate': 5.345132743362832e-06, 'epoch': 0.06}\n",
      "{'loss': 0.2009, 'grad_norm': 0.0013515031896531582, 'learning_rate': 4.4601769911504425e-06, 'epoch': 0.06}\n",
      "{'loss': 0.2075, 'grad_norm': 0.0024778719525784254, 'learning_rate': 3.5752212389380537e-06, 'epoch': 0.06}\n",
      "{'loss': 0.2052, 'grad_norm': 0.006263446062803268, 'learning_rate': 2.6902654867256636e-06, 'epoch': 0.06}\n",
      "{'loss': 0.2066, 'grad_norm': 0.002830411773175001, 'learning_rate': 1.8053097345132743e-06, 'epoch': 0.06}\n",
      "{'loss': 0.2098, 'grad_norm': 0.00465986505150795, 'learning_rate': 9.203539823008849e-07, 'epoch': 0.06}\n",
      "{'loss': 0.2141, 'grad_norm': 0.0016492604045197368, 'learning_rate': 3.539823008849558e-08, 'epoch': 0.06}\n",
      "100%|█████████████████████████████████████| 5700/5700 [6:51:30<00:00,  4.16s/it]/venv/main/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: c50a87c3-3f13-4ce9-9df9-fef4036c053d)') - silently ignoring the lookup for the file config.json in unsloth/llama-3-8b-bnb-4bit.\n",
      "  warnings.warn(\n",
      "/venv/main/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in unsloth/llama-3-8b-bnb-4bit - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "{'train_runtime': 24701.2822, 'train_samples_per_second': 3.692, 'train_steps_per_second': 0.231, 'train_loss': 0.21424890442898398, 'epoch': 0.06}\n",
      "100%|█████████████████████████████████████| 5700/5700 [6:51:41<00:00,  4.33s/it]\n",
      "Training complete.\n",
      "Saving LoRA adapter...\n",
      "\\nModel adapter saved to 'lora_model'.\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch --num_processes=1 train.py"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
